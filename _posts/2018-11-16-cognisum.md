---
layout: post
title:  "CogniSum"
date:   2018-11-16
excerpt: "Unsupervised Video Summarization inspired by Cognition"
project: true
tags: [deep, video, summarization, intelligent, cognition]
comments: false
---
<!-- 
![Architecture](/assets/img/cogsum.png)

<center><b>CogniSum:</b> Our Proposed Architecture.</center> -->

*We plan to submit a draft of our ideas to a relevant journal for consideration. Due to this, many details, including our official results have been redacted. This page will be updated after acceptance.*

During the summer of 2017, I got my start in the field of Deep Learning under the guidance of Prof. Anurag Mittal. Together with Mr. Vikram Singh (PhD Research Scholar at Computer Vision Lab, Indian Institute of Technology, Madras), we came up with a few novel ideas to greatly improve the accuracy of unsupervised networks on the summarization task.

## The Problem

Video Summarization is the task of succintly representing a video through a small subset of its frames. The size of this subset is usually specified by the user based on their needs. It is different from highlight extraction, where the aim is to find segments of the video where significant events occur.

## Core Principle

The inspiration for my work was mostly derived from [this wonderful paper](http://web.engr.oregonstate.edu/~sinisa/research/publications/cvpr17_summarization.pdf). The core idea proposed by the authors is the reconstruction principle. If one can recreate the original video using only a small subset of the frames, then this subset can be labelled as the summary of the video. To accomplish this, they make use of a generative framework (GAN) constructed using LSTMs (to ensure the retention of temporal information). Their results are highly encouraging for an unsupervised model, providing close competition to deep supervised models.

## Our Ideas

To gain some insight into the cognitive process in play, we surveyed 25 people. Details of this study will be made public after acceptance of the submitted paper. There are three novel ideas that helped us achieve exceptional performance on the summarization task:

1. Prior deep learning approaches to video summarization fail to take frame-level spatial information into consideration. If we consider multiple similar frames, the only measure to promote one frame over the others is content-richness. We evaluate content-richness of frames using Image Saliency.

2. Diversity among selected frames is key to obtaining good summaries. Other papers utilize a Determinantal Point Process (DPP) based formulation to enforce diversity. In addition to being computationally expensive, these are also not strictly intuitive. We decide to use a simple, novel variance-based sparsity loss. This loss works on the generated frame-level scores, and is hence extremely computationally efficient. 

3. For a summary to represent the entire video, the frames in it have to be spread throughout the temporal extent of the video. To enforce this, we make use of batch processing. We divide the video into snippets of 400 frames (chosen by experimentation), and summarize each such snippet.

## The Results

Our network comprehensively outperforms other unsupervised networks in terms of accuracy (measured either by the F1 score or Mean Average Precision). In addition, we also outperform supervised networks in most datasets by notable margins. We attribute this improvement to two reasons:

1. The carefully curated properties we choose to exploit are maybe intuitively sound. Since data is annotated by humans, the results suggest that to some extent, we succeed in our goal of emulating cognition. Our ablations demonstrate that the integration of saliency and the variance-based sparsity loss provide great improvements in the results.

2. The lack of large volumes of annotated training data is a fundamental issue in summarization. Considering the subjective nature of the problem and the broad variety of videos that exist, a supervised approach is not feasible. Additionally annotation for video summarization is an extremely tedious task, suggesting that unsupervised approaches hold the key to not only this problem, but many problems in the video domain.

## What's Next?

I am personally intrigued by the concept of the variance-based sparsity loss, and would like to see it being tried out in other problems where diversity is key. 

Lastly, video processing is an extremely computationally intensive task, especially when deep learned approached are to be used. I would like to see if the concept of working on a subset of frames (ergo, summarization) would yield accuracies close to the original. If it does, it would be a massive find. However, I don't believe the same intrinsic parameters can be used to summarize the video for all tasks, since the requirements for different tasks are different.  
